<!doctype html><html dir=ltr lang=ru data-theme=dark class="html theme--dark"><head><meta charset=utf-8><title>Дмитрий Кузнецов
|
Основы эффективного использования LLM в разработке
</title><meta name=generator content="Hugo 0.143.1"><meta name=viewport content="width=device-width,initial-scale=1,viewport-fit=cover"><meta name=author content="Дмитрий Кузнецов"><meta name=description content="Основы эффективного использования LLM (AI) в разработке"><link rel=stylesheet href=/css/anatole.min.937053fd3776aee2007dab0765e5ed4ad40e2c9634e7c7e7e2a8451c3b9591db.css integrity="sha256-k3BT/Td2ruIAfasHZeXtStQOLJY058fn4qhFHDuVkds=" crossorigin=anonymous><link rel=stylesheet href=/css/markupHighlight.min.73ccfdf28df555e11009c13c20ced067af3cb021504cba43644c705930428b00.css integrity="sha256-c8z98o31VeEQCcE8IM7QZ688sCFQTLpDZExwWTBCiwA=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/css/override.min.582ab7f7a3a821d6afb051ea8d64a1c1c5855671991b429ffe3603fcf937f6cf.css integrity="sha256-WCq396OoIdavsFHqjWShwcWFVnGZG0Kf/jYD/Pk39s8=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/404.min.8af5cba8db45fcd9bffa951d2cb07c1b34c55055091a151d255d39bc2ccfc919.css integrity="sha256-ivXLqNtF/Nm/+pUdLLB8GzTFUFUJGhUdJV05vCzPyRk=" crossorigin=anonymous media=screen><link rel=stylesheet href=/fontawesome/css/fontawesome.min.137b1cf3cea9a8adb7884343a9a5ddddf4280f59153f74dc782fb7f7bf0d0519.css integrity="sha256-E3sc886pqK23iENDqaXd3fQoD1kVP3TceC+3978NBRk=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/solid.min.e65dc5b48fb5f39b142360c57c3a215744c94e56c755c929cc3e88fe12aab4d3.css integrity="sha256-5l3FtI+185sUI2DFfDohV0TJTlbHVckpzD6I/hKqtNM=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/regular.min.6f4f16d58da1c82c0c3a3436e021a3d39b4742f741192c546e73e947eacfd92f.css integrity="sha256-b08W1Y2hyCwMOjQ24CGj05tHQvdBGSxUbnPpR+rP2S8=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/brands.min.e10425ad768bc98ff1fb272a0ac8420f9d1ba22f0612c08ff1010c95080ffe7e.css integrity="sha256-4QQlrXaLyY/x+ycqCshCD50boi8GEsCP8QEMlQgP/n4=" crossorigin=anonymous type=text/css><link rel="shortcut icon" href=/favicons/favicon.ico type=image/x-icon><link rel=apple-touch-icon sizes=180x180 href=/favicons/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicons/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicons/favicon-16x16.png><link rel=canonical href=https://kuznetsov.dev/posts/mastering-llm-in-development/><script type=text/javascript src=/js/anatole-header.min.f9132794301a01ff16550ed66763482bd848f62243d278f5e550229a158bfd32.js integrity="sha256-+RMnlDAaAf8WVQ7WZ2NIK9hI9iJD0nj15VAimhWL/TI=" crossorigin=anonymous></script><script type=text/javascript src=/js/redirect.min.0a6c00c0503228880893a70f3348da2a8ee321da80219f8b23ee33557045983b.js integrity="sha256-CmwAwFAyKIgIk6cPM0jaKo7jIdqAIZ+LI+4zVXBFmDs=" crossorigin=anonymous></script><script async defer data-domain=kuznetsov.dev src=https://analytics.flowmitry.com/js/script.file-downloads.outbound-links.tagged-events.js/js/script.file-downloads.outbound-links.tagged-events.js></script><script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script><meta name=twitter:card content="summary"><meta name=twitter:title content="Основы эффективного использования LLM в разработке"><meta name=twitter:description content="Основы эффективного использования LLM (AI) в разработке"><meta property="og:url" content="https://kuznetsov.dev/posts/mastering-llm-in-development/"><meta property="og:site_name" content="Культ Качества"><meta property="og:title" content="Основы эффективного использования LLM в разработке"><meta property="og:description" content="Основы эффективного использования LLM (AI) в разработке"><meta property="og:locale" content="ru"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-06-18T00:00:00+00:00"><meta property="article:modified_time" content="2025-06-18T00:00:00+00:00"><meta property="article:tag" content="Llm"><meta property="article:tag" content="Ai"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","articleSection":"posts","name":"Основы эффективного использования LLM в разработке","headline":"Основы эффективного использования LLM в разработке","alternativeHeadline":"","description":"
      Основы эффективного использования LLM (AI) в разработке


    ","license":"","inLanguage":"ru","isFamilyFriendly":"true","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/kuznetsov.dev\/posts\/mastering-llm-in-development\/"},"author":{"@type":"Person","name":"Дмитрий Кузнецов"},"creator":{"@type":"Person","name":"Дмитрий Кузнецов"},"accountablePerson":{"@type":"Person","name":"Дмитрий Кузнецов"},"copyrightHolder":{"@type":"Person","name":"Дмитрий Кузнецов"},"dateCreated":"2025-06-18T00:00:00.00Z","datePublished":"2025-06-18T00:00:00.00Z","dateModified":"2025-06-18T00:00:00.00Z","publisher":{"@type":"Organization","name":"Дмитрий Кузнецов","url":"https://kuznetsov.dev/","logo":{"@type":"ImageObject","url":"https:\/\/kuznetsov.dev\/favicons\/favicon-32x32.png","width":"32","height":"32"}},"image":[],"url":"https:\/\/kuznetsov.dev\/posts\/mastering-llm-in-development\/","wordCount":"1932","genre":[],"keywords":["llm","ai"]}</script></head><body class=body><div class=wrapper><aside class=wrapper__sidebar><div class="sidebar
."><div class=sidebar__content><div class=sidebar__introduction><img class=sidebar__introduction-profileimage src=/avatar-qq.jpg alt="profile picture"><div class=sidebar__introduction-title><a href=/>Дмитрий Кузнецов</a></div><div class=sidebar__introduction-description><p>Создавай. Улучшай. Автоматизируй.</p></div></div><ul class=sidebar__list><li class=sidebar__list-item><a href=https://www.linkedin.com/in/dmitry-kuznetsov-dev target=_blank rel="noopener me" aria-label=Linkedin title=Linkedin><i class="fab fa-linkedin-in fa-2x" aria-hidden=true></i></a></li><li class=sidebar__list-item><a href=https://github.com/flowmitry target=_blank rel="noopener me" aria-label=GitHub title=GitHub><i class="fab fa-github fa-2x" aria-hidden=true></i></a></li><li class=sidebar__list-item><a href=https://t.me/kuznetsov_dev target=_blank rel="noopener me" aria-label=Telegram title=Telegram><i class="fab fa-telegram-plane fa-2x" aria-hidden=true></i></a></li><li class=sidebar__list-item><a href=https://x.com/flowmitry target=_blank rel="noopener me" aria-label=Twitter title=Twitter><i class="fab fa-twitter fa-2x" aria-hidden=true></i></a></li></ul></div><footer class="footer footer__sidebar"><ul class=footer__list><li class=footer__item>&copy;
Дмитрий Кузнецов
2025</li></ul></footer><script type=text/javascript src=/js/medium-zoom.min.1248fa75275e5ef0cbef27e8c1e27dc507c445ae3a2c7d2ed0be0809555dac64.js integrity="sha256-Ekj6dSdeXvDL7yfoweJ9xQfERa46LH0u0L4ICVVdrGQ=" crossorigin=anonymous></script></div></aside><main class=wrapper__main><header class=header><div class=.><a role=button class=navbar-burger data-target=navMenu aria-label=menu aria-expanded=false><span aria-hidden=true class=navbar-burger__line></span>
<span aria-hidden=true class=navbar-burger__line></span>
<span aria-hidden=true class=navbar-burger__line></span></a><nav class=nav><ul class=nav__list id=navMenu><li class=nav__list-item><a href=/ title>Блог</a></li><li class=nav__list-item><a href=/posts/ title>Архив блога</a></li><li class=nav__list-item><a href=/about/ title>Обо мне</a></li></ul><ul class="nav__list nav__list--end"></ul></nav></div></header><div class="post
."><div class=post__content><h1>Основы Эффективного Использования LLM В Разработке</h1><p>Я постоянно ищу способы, как успевать больше за меньшее количество времени, и c восходом AI это наконец-то начало давать ощутимые плоды по части разработки; однако многие разработчики не видят (либо не хотят видеть) те возможности, которые открываются перед нами, и продолжают работать по старинке, невольно становясь кандидатами, которые могут и не выдержать конкуренции.</p><p>Ввиду того, что я все таки менеджер разработки, у меня много взаимодействия с разными разработчиками со всего мира, и я вижу их очень разное отношение к AI. В целом, готов его разделить на следующие категории:</p><ul><li>Вдохновлены и активно используют AI в работе (до 20%);</li><li>Проявляют интерес, пробуют, но используют AI на 20% (около 60%);</li><li>Скептики, которые либо попробовали и это не оправдало их ожиданий, либо даже не пробовали (около 20%).
Цифры довольно субъективны и скорее отражают мой личный опыт, нежели являются репрезентативными по всей индустрии.</li></ul><p>Есть исследования, которые показывают, что уровень проникновения AI в разработку - порядка 60%, что может и правда. Но пока эта цифра скорее отражает количество разработчиков использующих ChatGPT, Perplexity и подобные AI-powered сервисы без глубокого применения на практике. На деле, LLM в основном используются очень поверхностно, хотя потенциал там огромный.</p><h1 id=про-мой-опыт>Про мой опыт</h1><p>За последний год я попробовал разные редакторы и плагины для IDE powered by AI, но на данный момент остановился на Cursor AI, Github Copilot и JetBrains Junie как основных продуктах, которые применяю каждый день в разработке.</p><p>Я основательно не изучал ничего дополнительно, не читал как устроены LLM в деталях, но за счет переключения между разными инструментами, я начал замечать определенные паттерны, и в конечном счете пришел к определенным выводам, которые позволяют гораздо эффективнее использовать силу LLM&rsquo;ок в инди-разработке (не вижу причин, почему это не будет работать в компании, но об этом в следующих постах).</p><p>Выводы могут быть неточными с точки зрения Data Science (буду рад услышать фидбек), однако в упрощенном виде оно так и работает.
Надеюсь это будет полезно широкому кругу разработчиков, и в целом станет более понятно, как модели работают, почему глючат и что с этим можно сделать.</p><h1 id=на-что-жалуются-разработчики>На что жалуются разработчики</h1><p>В этой части я хочу разобрать несколько основных проблем, которые озвучивают разработчики, и объяснить, почему эти проблемы возникают.</p><p>Я буду разбирать только базовые принципы и ограничения, которые никуда не денутся в ближайшем будущем. А пока мы наблюдаем, как появляются различные способы обхода этих ограничений и используем это в своей работе.</p><h2 id=проблема-1-llm-галлюцинирует>Проблема №1: LLM галлюцинирует</h2><p>Мне понравилась фраза - &ldquo;LLM галлюцинирует в 100% случаев, но в 80% модель права&rdquo;. Любая мне известная LLM всегда стремится вам предоставить ответ, пусть даже и неверный. Фундаментально это сложно решить. Однако, стоит понимать, что есть разные причины галлюцинаций, и по отдельности их решить можно. В этом параграфе я разберу одну из таких причин.</p><p>Начну с теории. У каждой модели есть ограниченное <strong>context window</strong> - это объем текста, который LLM может одновременно учитывать или &ldquo;помнить&rdquo; при генерации ответа. Размер context window считают в токенах, но все усложняет то, что токен - величина не постоянная.
Для упрощения считают, что один токен (для английского языка) это 4 символа. То есть грубо говоря 100 токенов вмещает 75 английских слов.</p><p>Примеры размеров context window для разных моделей:
gpt-3 - 2,048 токенов (1,500 английских слов)
gpt-4o - 128,000 токенов (~96,000 английских слов)
gpt-4.1 - 1,000,000 (~750,000 английских слов).</p><p>Посчитать, сколько в вашем тексте токенов можно по ссылке <a href=https://platform.openai.com/tokenizer>https://platform.openai.com/tokenizer</a>. Важная ремарка - размер контекста не определяет напрямую на сколько &ldquo;умная&rdquo; модель. Но способность держать больше контекста точно идет моделям на пользу.</p><p>Когда вы переписываетесь с ChatGPT, в рамках каждого диалога у вас есть этот самый лимит. Если лимит превышен - модели начинают забывать (вытеснять) факты из диалога и галлюцинировать.</p><p>Создатели популярных моделей используют один способ для обхода этого ограничения. Чтобы не передавать весь диалог вместе с каждым новым сообщением к модели, они создают краткую выжимку с основными фактами после каждого полученного нового сообщения, и используют эту выжимку как контекст вместо передачи всего диалога. Это позволяет вам вести диалог дольше, но некоторые факты все же теряются. Выжимки создаются теми же моделями, у которых те же ограничения.
На ранних этапах зарождения LLM такой хак позволял увеличивать длину диалога. С ростом размера context window этот хак продолжает использоваться, но уже для оптимизации потребления вычислительных ресурсов (чем больше размер контекста, тем больше ресурсов требуется для обработки и генерации ответа).</p><p>Когда речь идет про разработку - принцип тот же. Однако я подозреваю, что Cursor и другие редакторы не создают выжимку из кода который вы шарите с ними, чтобы точность ответа была выше. И, как следствие, у вас быстрее заполняется контекстное окно.</p><h2 id=проблема-2-llm-использует-устаревшую-информацию>Проблема №2: LLM использует устаревшую информацию</h2><p>Каждая модель обучена на разных сетах данных, которые могут не включать в себя какие-то специфичные данные, такие как информация о последних версиях вашего любимого фреймворка. Да и в целом, могут и вовсе не знать о его существовании. Для не особо популярных технологий меньше информации в интернете, а следовательно и количество данных для обучения содержит меньше информации, что так же приводит к галлюцинациям, так как модели всегда стремятся дать какой-то ответ.</p><p>Модели широкого спектра очень популярны (GPT, Claude, Gemini и т.д.), но также набирают популярность и модели узкого спектра, натренированные на специализированных сетах данных. Например, модели конкретно для программирования (<a href=https://openai.com/index/introducing-codex/>OpenAI Codex</a>), или медицинские модели. Ввиду того, что обучаются такие модели на релевантном материале, их результаты работы - ощутимо лучше. Однако так же, как и в случае с общими моделями, их знания ограничиваются данными, включенными в обучение.</p><p>Представьте, что вас отправили на необитаемый остров на 20 лет. Мир уже шагнул вперед, а ваши знания застыли на определенной точке. С моделями так же. Их периодически требуется переобучать/дообучать на новых данных, чтобы они были релевантными.</p><p>Также важно упомянуть, что модели не имеют доступа к интернету, однако все основные игроки на рынке AI внедрили возможность поиска в интернете и использования результатов поиска в рамках вашего диалога. Это позволяет получать доступ к свежей информации, даже несмотря на то, что модель была обучена на старых данных.
Как наверняка это реализовано - сказать сложно, но один из способов этой реализации - это разработка агента на базе <a href=https://modelcontextprotocol.io/>MCP</a> протокола. В детали вдаваться тут не буду, но замечу, что проще всего думать, что MCP - это протокол, по которому модели могут взаимодействовать с внешними сервисами, понимать список их функций и выполнять их.</p><h2 id=проблема-3-каждый-раз-приходится-объяснять-все-по-новой>Проблема №3: Каждый раз приходится объяснять все по новой</h2><p>Модель сама по себе не имеет памяти. Она не может записывать то, что вы в нее отправляете. Однако все крупные игроки внедрили эмуляцию &ldquo;памяти&rdquo; в свои официальные клиенты. Например, ChatGPT, по ходу ваших диалогов своими моделями определяет &ldquo;факты&rdquo; о вас, которые имеет смысл сохранить. А потом использует эти факты как часть контекста для ваших чатов. Это и создает иллюзию того, что модель кое-что знает о вас.
Также, память можно реализовать с помощью упомянутого ранее <a href=https://modelcontextprotocol.io/>MCP</a> или <a href=https://en.wikipedia.org/wiki/Retrieval-augmented_generation>RAG</a>.</p><p>Так что, если при разработке вы устали повторять одно и тоже, придется найти механизм эмуляции &ldquo;памяти&rdquo; для модели. И такой способ есть, об этом будет ниже.</p><h1 id=как-решать-эти-проблемы-в-cursor-ai>Как решать эти проблемы в Cursor AI</h1><p>Описанные проблемы и способы обхода могут казаться очевидными и я уверен, что многие уже знакомы с ними так или иначе. Но с учетом того, что я регулярно объясняю одно и то же разным людям, есть пустота, которую хочется заполнить.</p><p>Давайте на примере Cursor AI разберемся, как можно решать перечисленные выше проблемы конкретно для разработки.</p><h2 id=1-ограничение-context-window>1. Ограничение Context Window</h2><p>Любые ресурсы ограничены. И так же стоит относится к LLM, здесь нет никакой магии.</p><p>Вот несколько практик, которые помогут вам сталкиваться с галлюцинированием реже из-за размера Context Window:</p><h3 id=создавайте-новые-диалоги-как-можно-чаще>Создавайте новые диалоги как можно чаще</h3><p>Старайтесь следовать принципу: 1 проблема/задача/вопрос = 1 диалог.
Когда диалог получается большим - просите создать выжимку для нового диалога и копируйте ее в новый диалог.
Также вспомните ситуацию, когда во время разговора вы резко меняли тему и ваш собеседник отвечал что-то не по теме. С моделями такое тоже случается, когда вы смешиваете вопросы, поэтому стоит создавать новый диалог для обсуждения чего-то не связанного с предыдущей темой.</p><h3 id=передавайте-в-диалог-только-необходимый-контекст>Передавайте в диалог только необходимый контекст</h3><p>Вы можете выбрать весь проект в качестве контекста и получить хороший результат. И такое возможно, но в основном только для маленьких проектов. Для средних и больших, увы, придется выбирать контекст вручную.</p><h3 id=переключайтесь-на-модель-с-большим-context-window>Переключайтесь на модель с бОльшим context window</h3><p>Иногда требуется загрузить больше контекста, и в этом случае вы можете попробовать переключиться на модель с большим размером context window. Но для этого придется познакомиться с теми моделями, которые представлены в Cursor и изучить их характеристики. <a href=https://platform.openai.com/docs/models>Тут</a> вы можете познакомится с описанием моделей от OpenAI.</p><h3 id=ограничьте-индексацию-проекта>Ограничьте индексацию проекта</h3><p>У Cursor есть механизм <code>.cursorignore</code>. Он позволяет Cursor&rsquo;у игнорировать и не индексировать определенные части вашего проекта. Тем самым сужая scope проекта и увеличивая скорость при поиске. Это актуально для монорепозиториев и больших проектов. Подробнее <a href=https://docs.cursor.com/context/codebase-indexing>тут</a>.</p><h2 id=2-доступ-к-актуальной-информации>2. Доступ к актуальной информации</h2><p>Досадно, когда ты уже используешь React 19, а Cursor подсказывает только по 18 версии, так как модель была обучена на данных, когда 19 версии еще не было.</p><p>Разработчики Cursor предлагают несколько способов, как решить эту проблему.</p><h3 id=используйте-тег-docs>Используйте тег @docs</h3><p>При использовании тега <code>@docs &lt;url></code> и предоставления ему ссылки на актуальную документацию, Cursor запустит индексацию страниц, и по завершении будет помогать вам уже с полным пониманием новых фишек фреймворка.</p><p>Точно так же решается проблема с внутренней документацией. Строго говоря, вы можете скормить и swagger документацию от вашей внутренней апишки.</p><h3 id=используйте-тег-web>Используйте тег @web</h3><p>Для поиска другой информации в сети, не покидая вашей IDE, воспользуйтесь тегом <code>@web запрос</code>. Казалось бы ничего особенного, загуглить можно и без этого, но тут можно работать с этой информацией в рамках того же чата. Так могут подтягиваться решения со StackOverflow, например.</p><h3 id=используйте-mcp>Используйте MCP</h3><p>Cursor позволяет подключить любые MCP, которые потом будут использоваться в ваших чатах при необходимости.
Один из примеров - <a href=https://github.com/upstash/context7>MCP context7</a> который как раз помогает получать актуальную документацию.</p><h2 id=3-учим-cursor-запоминать>3. Учим Cursor запоминать</h2><p>Что может быть интереснее, чем каждый раз объяснять Cursor, как у вас организованы файлы и методы? Или как создать новый API в вашем проекте? Или стоит ли использовать внешние библиотеки, либо ограничиваться стандартной библиотекой? Или может как устроена межсервисная архитектура и в чем основная цель проекта?</p><h3 id=ссылаемся-на-старые-чаты>Ссылаемся на старые чаты</h3><p>Cursor недавно добавил возможность ссылаться на ваши старые чаты. Это можно использовать в формате &ldquo;сделай так же, как и в том чате&rdquo;, либо как продолжение связанной задачи.</p><h3 id=ссылаемся-на-файлы>Ссылаемся на файлы</h3><p>Неочевидно, но тоже можно использовать в формате &ldquo;проанализируй этот файл/папку и сделай так же&rdquo;.</p><h3 id=cursor-rules>Cursor Rules</h3><p>У Cursor и других IDE уже есть ответ по части организации памяти - <a href=https://docs.cursor.com/context/rules>cursor rules</a>.
Это файл (либо набор файлов в случае Cursor), которые передаются автоматически (или вручную) в рамках вашего контекста. Таким образом, при наличии документации в формате <code>cursor rules</code>, вам не придется больше повторять себя, а Cursor будет ориентироваться на ваши существующие стандарты.</p><p>Да, это по факту документация, и да, ее нужно писать. Cursor, как и junior разработчик нуждается в объяснениях и руководстве. Хотя некоторые агенты (например JetBrains Junie) делают успехи в анализе существующего кода, пытаясь сделать так же. Такой подход хоть и удобен, но не всегда эффективен, особенно, когда работаешь со старым кодом.</p><p>Хорошие новости в том, что всегда можно попросить Cursor сгенерировать и сохранить правила на основе вашего чата (прям так и спросить). А буквально несколько дней назад они реализовали команду <code>/Generate Cursor Rules</code>, которая именно это и делает. Да, результат придется поревьювить и, возможно, попросить пару раз внести правки, но это значительно сокращает трудозатраты на описание.</p><p>И как приятное дополнение - новички в вашей команде смогут читать ваши <code>cursor rules</code> как документацию и стандарты.</p><h1 id=в-заключении>В заключении</h1><p>В заключении, предлагаю вам подумать и ответить на следующие вопросы:</p><ol><li>Сколько токенов в этой статье?</li><li>Как потенциально себя будет вести Cursor при работе с файлом на 10 тысяч строк?</li><li>Что требуется сделать, чтобы эффективно провести рефакторинг с помощью Cursor?</li><li>Как заставить Cursor запускать линтеры или сборку в конце каждого чата и исправлять ошибки автоматически?</li></ol><p>Эффективное использование вполне возможно при понимании, какие ограничения есть у LLM.</p></div><div class=post__footer><span><a class=tag href=/tags/llm/>llm</a><a class=tag href=/tags/ai/>ai</a></span></div></div></main></div><footer class="footer footer__base"><ul class=footer__list><li class=footer__item>&copy;
Дмитрий Кузнецов
2025</li></ul></footer><script type=text/javascript src=/js/medium-zoom.min.1248fa75275e5ef0cbef27e8c1e27dc507c445ae3a2c7d2ed0be0809555dac64.js integrity="sha256-Ekj6dSdeXvDL7yfoweJ9xQfERa46LH0u0L4ICVVdrGQ=" crossorigin=anonymous></script></body></html>